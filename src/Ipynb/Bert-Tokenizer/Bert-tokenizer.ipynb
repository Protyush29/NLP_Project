{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u1Mclb-2HVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYQvFsf12a9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5adaece2-f9af-4eb5-964a-86fd2c137795"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(hub.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQT6-WzV2oUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "26adf16a-c482-4878-ba95-01cd414bcf69"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epnh5dlM2woS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3345aab-9986-477c-fa6f-c191af626559"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRj6WPYU21_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuq9lH_H234d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-q3o-km3OVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_test = pd.read_csv('Test.csv')\n",
        "movie_train = pd.read_csv('Train.csv')\n",
        "movie_valid = pd.read_csv('Valid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm9FPuuJ4Bnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42b8da22-07ac-4be4-c055-0b898f7d9887"
      },
      "source": [
        "movie_test.columns.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['text', 'label'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfg1VU8l4Udi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48d7b0a4-d43a-43fb-f718-fe98488b25cf"
      },
      "source": [
        "movie_test.label.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKHLVAH55K56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6dcea2ad-427c-4671-9bd7-280b42683bc4"
      },
      "source": [
        "print(movie_train.shape)\n",
        "print(movie_test.shape)\n",
        "print(movie_valid.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 2)\n",
            "(5000, 2)\n",
            "(5000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl7STTYH5wNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35o3Kbub6Knl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = []\n",
        "sentences = list(movie_train['text'])\n",
        "for sen in sentences:\n",
        "    reviews.append(preprocess_text(sen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFwOfDm-6bSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7ee04810-f3dc-4a9a-b996-f54586e233a8"
      },
      "source": [
        "reviews[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can believe people are looking for plot in this film This is Laural and Hardy Lighten up already These two were riot Their comic genius is as funny today as it was years ago Not filthy word out of either mouth and they were able to keep audiences in stitches Their comedy wasn sophisticated by any stretch If whoopee cushion can make you grin there no reason to watch any of the stuff these guys did It was simpler time and people laughed at stuff that was funny without plot guess it takes simple mind to enjoy this stuff so qualify Two man comedy teams don compute We re just too sophisticated Aren we fortunate '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF0DjS2x6uvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NssNPpC86yxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = movie_train['label']\n",
        "y = np.array(list(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYbkvjOv7gu5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d92c429-70dc-4623-fa15-a5a7f68ba381"
      },
      "source": [
        "y[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAJ5Lk337zH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA55xkn579_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_reviews(text_reviews):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))\n",
        "\n",
        "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Udyc3W8YCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_with_len = [(review, y[i])\n",
        "                 for i, review in enumerate(tokenized_reviews)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jx7_Hd_9-2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: reviews_with_len, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtJxntXH-O-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny1D_rox-iF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e494e43b-dd3f-490f-96ad-88fb2f030042"
      },
      "source": [
        "import math\n",
        "TOTAL_BATCHES = math.ceil(len(reviews_with_len) / BATCH_SIZE)\n",
        "print(TOTAL_BATCHES)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "print(TEST_BATCHES)\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250\n",
            "125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFKAXzN1_nHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaH7T2duCaP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5\n",
        "\n",
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsXY4j7xC_J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if OUTPUT_CLASSES == 2:\n",
        "    text_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSIdHO-kDKd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "79571067-b22e-4012-cca0-45397d0edbaa"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - 882s 784ms/step - loss: 0.3182 - accuracy: 0.8586\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 890s 791ms/step - loss: 0.1277 - accuracy: 0.9528\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 905s 804ms/step - loss: 0.0680 - accuracy: 0.9756\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 883s 785ms/step - loss: 0.0327 - accuracy: 0.9885\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 910s 809ms/step - loss: 0.0168 - accuracy: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d3de660f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpD41VeqDLRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "40004292-3517-41d9-d76c-7fbc467eca22"
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 26s 207ms/step - loss: 0.8215 - accuracy: 0.8585\n",
            "[0.8214634656906128, 0.8585000038146973]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR18qb6r-t6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy\",metrics.accuracy_score(data_test[0], df_results[0]))\n",
        "print(\"F1-Score\",metrics.f1_score(data_test[0], df_results[0],average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}